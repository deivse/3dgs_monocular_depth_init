{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_configured_monodepth_model() missing 1 required positional argument: 'cfg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m rgb \u001b[38;5;241m=\u001b[39m rgb[\u001b[38;5;28;01mNone\u001b[39;00m, :, :, :]\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_configured_monodepth_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m model\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_configured_monodepth_model() missing 1 required positional argument: 'cfg'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import Metric3D\n",
    "\n",
    "\n",
    "\n",
    "# Add the Metric3D directory to the Python path\n",
    "sys.path.append(os.path.join(os.getcwd(), 'Metric3D'))\n",
    "\n",
    "from Metric3D.mono.model.monodepth_model import get_configured_monodepth_model\n",
    "\n",
    "# Prepare data\n",
    "rgb_file = 'Metric3D/data/kitti_demo/rgb/0000000050.png'\n",
    "depth_file = 'Metric3D/data/kitti_demo/depth/0000000050.png'\n",
    "intrinsic = [707.0493, 707.0493, 604.0814, 180.5066]\n",
    "gt_depth_scale = 256.0\n",
    "rgb_origin = cv2.imread(rgb_file)[:, :, ::-1]\n",
    "\n",
    "# Adjust input size to fit pretrained model\n",
    "input_size = (616, 1064)  # for vit model\n",
    "# input_size = (544, 1216)  # for convnext model\n",
    "h, w = rgb_origin.shape[:2]\n",
    "scale = min(input_size[0] / h, input_size[1] / w)\n",
    "rgb = cv2.resize(rgb_origin, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_LINEAR)\n",
    "intrinsic = [intrinsic[0] * scale, intrinsic[1] * scale, intrinsic[2] * scale, intrinsic[3] * scale]\n",
    "padding = [123.675, 116.28, 103.53]\n",
    "h, w = rgb.shape[:2]\n",
    "pad_h = input_size[0] - h\n",
    "pad_w = input_size[1] - w\n",
    "pad_h_half = pad_h // 2\n",
    "pad_w_half = pad_w // 2 \n",
    "rgb = cv2.copyMakeBorder(rgb, pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half, cv2.BORDER_CONSTANT, value=padding)\n",
    "pad_info = [pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half]\n",
    "\n",
    "# Normalize\n",
    "mean = torch.tensor([123.675, 116.28, 103.53]).float()[:, None, None]\n",
    "std = torch.tensor([58.395, 57.12, 57.375]).float()[:, None, None]\n",
    "rgb = torch.from_numpy(rgb.transpose((2, 0, 1))).float()\n",
    "rgb = torch.div((rgb - mean), std)\n",
    "rgb = rgb[None, :, :, :].cuda()\n",
    "\n",
    "# Load the model\n",
    "model = get_configured_monodepth_model(pretrain=True)\n",
    "model.cuda().eval()\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    pred_depth, confidence, output_dict = model.inference({'input': rgb})\n",
    "\n",
    "# Unpad\n",
    "pred_depth = pred_depth.squeeze()\n",
    "pred_depth = pred_depth[pad_info[0]:pred_depth.shape[0] - pad_info[1], pad_info[2]:pred_depth.shape[1] - pad_info[3]]\n",
    "\n",
    "# Upsample to original size\n",
    "pred_depth = torch.nn.functional.interpolate(pred_depth[None, None, :, :], rgb_origin.shape[:2], mode='bilinear').squeeze()\n",
    "\n",
    "# De-canonical transform\n",
    "canonical_to_real_scale = intrinsic[0] / 1000.0  # 1000.0 is the focal length of canonical camera\n",
    "pred_depth = pred_depth * canonical_to_real_scale  # now the depth is metric\n",
    "pred_depth = torch.clamp(pred_depth, 0, 300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
